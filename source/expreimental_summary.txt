Tier 1
3. EfficientNet-s: Best val Acc: 0.8904, dropped comparing to 0.899543; Test Acc: 0.8846 dropped comparing to 0.8864; more stable from training and validation curve. 

Drop down learning rate from 0.0001 to 0.00005, Best val Acc dropped (0.8895), Early stopping triggered after 6 epochs. 

Tier 2
4. Enhanced data augmentaion. resNet18: Best val Acc: 0.8968 (from 0.8877)  Test Accuracy: 0.8828 (from 0.8810); 
                                        * increase weight_decay (from 1e-4 to 0.01) with AdamW() and add dropout layers 0.4: (not good) Best val Acc: 0.87, Test Acc: 0.85
                            DenseNet121: Best val Acc: 0.9105 (from 0.903196) Test Accuracy: 0.8846 (from 0.901099); Best train Acc: 0.97 -> huge gap, overfitting
                                        * increase weight_decay (from 1e-4 to 1e-3): Best val Acc: 0.9151 Test Accuracy: 0.8938 Best train Acc: 0.98 -> still overfitting
                                        * AdamW() weight_decay = 0.01 (stronger regularization): Best val Acc: 0.9132 Test Accuracy: 0.8846 Best train Acc: 0.97
                                        * increase weight_decay (from 1e-4 to 0.01) with AdamW() and add dropout layers 0.4: Best val Acc: 0.9196, Test Acc: 0.9048, Best train Acc: 0.9651
                                        * Focal Loss for Class Imbalance: Best val Acc: 0.9151, Test Accuracy: 0.8974, Train Acc: 0.9754
                                        * Stochastic Weight Averaging: abandon
                            EfficientNet-V2-S: Best val Acc: 0.9470, Test Accuracy: 0.9304
                                        * increase weight_decay (0.02) with AdamW() and add dropout layers 0.5: Best val Acc: 0.9470, Test Accuracy: 0.9286




I did some experiments for hyperparameters tuning, loss functions choosen. And I want to add this process to ACCURACY_IMROVEMENT.md.

Firstly, I trained models using basic data augmentation in insect_classification_transfer_learning.ipynb (benchmark models). 
Then I use enhanced data augmentation techniques and retrained them in accuracy_impro.ipynb and get good improvement overall. 
DenseNet121 had the best performance so I tried variety weight_decay parameters and loss functions. It went out that increase weight_decay (from 1e-4 to 0.01) with AdamW() and add dropout layers 0.4
improved DenseNet121 most. Then I applied same modification to resNet18 and EfficientNet but it didn't perform well so I kept then same. 





